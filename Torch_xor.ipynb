{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import os \n",
    "from torch.nn import parameter\n",
    "Parameter = parameter.Parameter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, Function\n",
    "import torchvision.transforms as T\n",
    "from catch import Catch\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = []\n",
    "        self.discount = discount\n",
    "        self.current_memory = []\n",
    "\n",
    "    def push(self, states, game_over, ball_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.current_memory.append([states, game_over])\n",
    "        if ball_over or game_over:\n",
    "            self.memory.append(self.current_memory)\n",
    "            if len(self.memory) > self.max_memory:\n",
    "                del self.memory[0]\n",
    "            self.current_memory = []\n",
    "\n",
    "\n",
    "    def sample(self, model, batch_size=1):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "\n",
    "        Oin = None\n",
    "        Otar = None\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=min(len_memory, batch_size))):\n",
    "\n",
    "            mems = self.memory[idx]\n",
    "            for mem in mems:\n",
    "                state_t, action_t, reward_t, state_tp1 = mem[0]\n",
    "                game_over = mem[1]\n",
    "                inputs = state_t\n",
    "                temp_state_t = Variable(torch.from_numpy(state_t).type(Tensor))\n",
    "                temp_state_tp1 = Variable(torch.from_numpy(state_tp1).type(Tensor))\n",
    "                \n",
    "                # There should be no target values for actions not taken.\n",
    "                # Thou shalt not correct actions not taken #deep\n",
    "                targets = model(temp_state_t).data.numpy()[0]\n",
    "                Q_sa = model(temp_state_tp1).data.max(1)[0].view(1, 1).numpy()[0][0]\n",
    "                \n",
    "                if game_over:  # if game_over is True\n",
    "                    targets[action_t] = reward_t\n",
    "                else:\n",
    "                    # reward_t + gamma * max_a' Q(s', a')\n",
    "                    targets[action_t] = reward_t + self.discount * Q_sa\n",
    "                if Oin is None:\n",
    "                    Oin = inputs\n",
    "                    Otar = np.expand_dims(targets, axis=0)\n",
    "                else:\n",
    "                    Oin = np.concatenate([Oin, inputs], axis=0)\n",
    "                    Otar = np.concatenate([Otar, np.expand_dims(targets, axis=0)], axis=0)\n",
    "        return Oin, Otar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Queue(nn.Module):\\n    def __init__(self, input_features, out_features, queue_size):\\n        super(Queue, self).__init__()\\n        self.input_features = input_features\\n        self.queue_size = queue_size\\n        # Setup the queue\\n        if type(input_features) is list:\\n            self.queue = np.zeros([queue_size]+input_features)\\n        else:\\n            self.queue = np.zeros([queue_size]+[input_features])\\n\\n\\n    def forward(self, inputs):\\n        # The actual call that is run when you pass data into this model. \\n        output = None\\n        for temp in inputs.data.numpy():\\n            #print(self.queue[:, 1:].size(), torch.from_numpy(temp).type(Tensor).unsqueeze(0).size())\\n            self.queue = np.concatenate((self.queue, np.expand_dims(temp, 0)), 0)\\n            self.queue = self.queue[1:]\\n            if output is None:\\n                output = np.expand_dims(self.queue, 0)\\n            else:\\n                output = np.concatenate((output, np.expand_dims(self.queue, 0)), 0)\\n        # Return \\n        output = Variable(torch.from_numpy(output).type(Tensor))\\n        #output.requires_grad = False\\n        output = output.view(output.size()[0], -1) # FLATTEN and make into a Variable for Torch\\n        return output\\n            \\n    # reunit the queue\\n    def _reset_queue(self):\\n        if type(self.input_features) is list:\\n            self.queue = np.zeros([self.queue_size]+self.input_features)\\n        else:\\n            self.queue = np.zeros([self.queue_size]+[self.input_features])\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class fifo_queue(Function):\n",
    "    def forward(self, old_queue, inputs):\n",
    "        outputs = None\n",
    "        queue = old_queue.clone()\n",
    "        for temp in Variable(inputs).data.numpy():\n",
    "            queue = torch.cat((queue, torch.from_numpy(temp).type(Tensor).unsqueeze(0)), 0)\n",
    "            queue = queue[1:]\n",
    "            if outputs is None:\n",
    "                outputs = queue.clone().unsqueeze(0)\n",
    "            else:\n",
    "                outputs = torch.cat((outputs, queue.unsqueeze(0)), 0)\n",
    "        \n",
    "        self.mark_non_differentiable(outputs)\n",
    "        return outputs, queue\n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        return None\n",
    "\n",
    "\n",
    "class Queue(nn.Module):\n",
    "    def __init__(self, input_features, out_features, queue_size, bias=None):\n",
    "        super(Queue, self).__init__()\n",
    "        # Setup the Queue\n",
    "        self.input_features = input_features\n",
    "        self.queue_size = queue_size\n",
    "        self._reset_queue()\n",
    "        \n",
    "        # Init the queue function\n",
    "        self.fifo_queue = fifo_queue()\n",
    "\n",
    "        # Setup the weights to be used in the linear function later. \n",
    "        self.weight = Parameter(torch.Tensor(out_features, input_features*queue_size))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        # Init the parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        # The actual call that is run when you pass data into this model. \n",
    "        \n",
    "        x = self.fifo_queue(Variable(self.queue), inputs)\n",
    "        output = x[0]\n",
    "        self.queue = x[1].data\n",
    "        #output, self.queue \n",
    "        # Return \n",
    "        output = output.view(output.size()[0], -1)\n",
    "        return F.linear(output, self.weight, self.bias)\n",
    "            \n",
    "    # reunit the queue\n",
    "    def _reset_queue(self):\n",
    "        if type(self.input_features) is list:\n",
    "            queue = torch.zeros([self.queue_size]+self.input_features)\n",
    "        else:\n",
    "            queue = torch.zeros([self.queue_size]+[self.input_features])\n",
    "        self.queue = queue\n",
    "        \n",
    "        self.saved_queue = None\n",
    "        #self.register_buffer(\"queue\", queue)\n",
    "        self.queue.requires_grad = False\n",
    "        \n",
    "    def save_queue(self):\n",
    "        self.saved_queue = self.queue.clone()\n",
    "        \n",
    "    def load_queue(self):\n",
    "        self.queue = self.saved_queue.clone()\n",
    "        self.saved_queue = None\n",
    "\n",
    "\"\"\"class Queue(nn.Module):\n",
    "    def __init__(self, input_features, out_features, queue_size):\n",
    "        super(Queue, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.queue_size = queue_size\n",
    "        # Setup the queue\n",
    "        if type(input_features) is list:\n",
    "            self.queue = np.zeros([queue_size]+input_features)\n",
    "        else:\n",
    "            self.queue = np.zeros([queue_size]+[input_features])\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The actual call that is run when you pass data into this model. \n",
    "        output = None\n",
    "        for temp in inputs.data.numpy():\n",
    "            #print(self.queue[:, 1:].size(), torch.from_numpy(temp).type(Tensor).unsqueeze(0).size())\n",
    "            self.queue = np.concatenate((self.queue, np.expand_dims(temp, 0)), 0)\n",
    "            self.queue = self.queue[1:]\n",
    "            if output is None:\n",
    "                output = np.expand_dims(self.queue, 0)\n",
    "            else:\n",
    "                output = np.concatenate((output, np.expand_dims(self.queue, 0)), 0)\n",
    "        # Return \n",
    "        output = Variable(torch.from_numpy(output).type(Tensor))\n",
    "        #output.requires_grad = False\n",
    "        output = output.view(output.size()[0], -1) # FLATTEN and make into a Variable for Torch\n",
    "        return output\n",
    "            \n",
    "    # reunit the queue\n",
    "    def _reset_queue(self):\n",
    "        if type(self.input_features) is list:\n",
    "            self.queue = np.zeros([self.queue_size]+self.input_features)\n",
    "        else:\n",
    "            self.queue = np.zeros([self.queue_size]+[self.input_features])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Queued_DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_actions, queue_size):\n",
    "        super(Queued_DQN, self).__init__()\n",
    "        self.queue = Queue(input_size, input_size*queue_size, queue_size)\n",
    "        #self.lin1 = nn.Linear(input_size*queue_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lin3 = nn.Linear(hidden_size, 1)\n",
    "        self.output_shape = [1]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Replace queue with another Linear and you \n",
    "        # have the same network we use for everything else. \n",
    "        # This is the model\n",
    "        x = F.relu(self.queue(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "    def reset_queue(self):\n",
    "        self.queue._reset_queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class DQN(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_actions):\\n        super(DQN, self).__init__()\\n        self.lin1 = nn.Linear(input_size, hidden_size)\\n        self.lin2 = nn.Linear(hidden_size, hidden_size)\\n        self.lin3 = nn.Linear(hidden_size, num_actions)\\n        self.output_shape = [1, num_actions]\\n        \\n    def forward(self, x):\\n        x = F.relu(self.lin1(x))\\n        x = F.relu(self.lin2(x))\\n        return self.lin3(x)'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lin3 = nn.Linear(hidden_size, num_actions)\n",
    "        self.output_shape = [1, num_actions]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        return self.lin3(x)\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_size = 2\n",
    "\n",
    "epochs = 1000\n",
    "input_size = 1\n",
    "batch_size = 1000\n",
    "model = Queued_DQN(1, 50, 1, 1)\n",
    "#model = DQN(input_size, input_size, 3)\n",
    "optimizer = optim.Adagrad(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 100\n",
      "epoch 200\n",
      "epoch 300\n",
      "epoch 400\n",
      "epoch 500\n",
      "epoch 600\n",
      "epoch 700\n",
      "epoch 800\n",
      "epoch 900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for y in range(epochs):\n",
    "    training = np.random.randint(0, 2, batch_size)\n",
    "    previous = 0\n",
    "    expected_targets = []\n",
    "    for x in training:\n",
    "        expected_targets.append(int(np.logical_xor(previous, x)))\n",
    "        previous = x\n",
    "    expected_targets = np.array(expected_targets)\n",
    "    training = np.expand_dims(training, -1)\n",
    "    targets = model(Variable(torch.from_numpy(training).type(Tensor)))\n",
    "    expected_targets = Variable(torch.from_numpy(expected_targets).type(Tensor))\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.mse_loss(targets, expected_targets)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if y% 100 == 0:\n",
    "        print(\"epoch\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing = np.random.randint(0, 2, batch_size)\n",
    "previous = [0]\n",
    "expected_targets = []\n",
    "for x in testing:\n",
    "    expected_targets.append(int(np.logical_xor(previous, x)))\n",
    "    previous = x\n",
    "expected_targets = np.array(expected_targets)\n",
    "testing = np.expand_dims(testing, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = model(Variable(torch.from_numpy(testing).type(Tensor)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572313]\n",
      "[1] 0 [ 0.49572313]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[0] 0 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[1] 0 [ 0.49572316]\n",
      "[0] 1 [ 0.49928012]\n",
      "[1] 1 [ 0.49572313]\n"
     ]
    }
   ],
   "source": [
    "for x in zip(testing, expected_targets, targets):\n",
    "    print(x[0], x[1], x[2].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logical_xor(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKSPACE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = torch.load(\"configs//\" + test_name + \".ptm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = action.data.max(1)[0].view(1, 1).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = exp.sample(model, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_tm1 = c.observe(flatten=True, expand_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = torch.from_numpy(input_tm1).type(Tensor)\n",
    "temp = Variable(temp)\n",
    "q = model2(temp).data.max(1)[1].view(1, 1).numpy()\n",
    "action = q[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 11\n",
    "width=13\n",
    "m = False\n",
    "q_size = 1\n",
    "i=0\n",
    "test_name = \"RUN\" + str(i) + \"_Mask_\" + str(int(m)) + \"_13x11_Q\" + str(q_size)+\"_MLP\"\n",
    "\n",
    "c = Catch(screen_height=height, screen_width=width, output_buffer_size=q_size,\n",
    "          game_over_conditions = {'ball_deletions': 10}, mask=m, ball_spawn_rate=11,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queue = Queued_DQN(input_size, input_size*4, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5667  0.2255  0.2753\n",
       "[torch.FloatTensor of size 1x3]"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.act(1)\n",
    "bb(Variable(torch.from_numpy(c.observe(flatten=True, expand_dim=True)).type(Tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = Variable(torch.from_numpy(c.observe(flatten=True, expand_dim=True)).type(Tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
